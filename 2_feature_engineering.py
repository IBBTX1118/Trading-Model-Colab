# æª”å: 2_feature_engineering_enhanced.py
# ç‰ˆæœ¬: 8.0 (æ•´åˆåµéŒ¯ç‰ˆ)
# æè¿°: åŸ·è¡Œå®Œæ•´çš„ç‰¹å¾µå·¥ç¨‹å¾Œï¼Œè‡ªå‹•å°ä¸€å€‹æ¨£æœ¬å¸‚å ´é€²è¡Œæ•¸æ“šå¥åº·åº¦è¨ºæ–·ã€‚

import logging
import sys
from pathlib import Path
from typing import Dict, Optional
import pandas as pd
import numpy as np
from finta import TA
from collections import defaultdict

# ==============================================================================
#                      1. é…ç½®é¡åˆ¥
# ==============================================================================
class Config:
    INPUT_BASE_DIR = Path("Output_Data_Pipeline_v2/MarketData")
    OUTPUT_BASE_DIR = Path("Output_Feature_Engineering/MarketData_with_Combined_Features_v3")
    LOG_LEVEL = "INFO"
    TIMEFRAME_ORDER = ['D1', 'H4', 'H1']
    EVENTS_FILE_PATH = Path("economic_events.csv")
    GLOBAL_FACTORS_FILE_PATH = Path("global_factors.parquet")
    DEBUG_MARKET_SAMPLE = "EURUSD_sml_H4" # æŒ‡å®šä¸€å€‹ç”¨æ–¼è‡ªå‹•åµéŒ¯çš„æ¨£æœ¬å¸‚å ´

# ==============================================================================
#                      2. ç‰¹å¾µå·¥ç¨‹ä¸»é¡åˆ¥
# ==============================================================================
class FeatureEngineer:
    # ... æ­¤è™•çš„ FeatureEngineer class çš„æ‰€æœ‰å…§å®¹èˆ‡æ‚¨æä¾›çš„
    # ... ibbtx1118/.../2_feature_engineering.py å®Œå…¨ç›¸åŒï¼Œç‚ºäº†ç°¡æ½”æ­¤è™•çœç•¥ã€‚
    # ... è«‹å°‡æ‚¨åŸæœ¬çš„ FeatureEngineer class å…§å®¹å®Œæ•´è¤‡è£½åˆ°é€™è£¡ã€‚
    def __init__(self, config: Config):
        self.config = config
        self.logger = self._setup_logger()
        self.config.OUTPUT_BASE_DIR.mkdir(parents=True, exist_ok=True)

    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(self.config.LOG_LEVEL.upper())
        if not logger.hasHandlers():
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            sh = logging.StreamHandler(sys.stdout)
            sh.setFormatter(formatter)
            logger.addHandler(sh)
        return logger

    def _add_global_factor_features(self, df: pd.DataFrame, factors_df: Optional[pd.DataFrame]) -> pd.DataFrame:
        """æ·»åŠ å…¨å±€å› å­ç‰¹å¾µ (DXY, VIXç­‰)"""
        if factors_df is None or factors_df.empty:
            return df
        df_merged = pd.merge_asof(df.sort_index(), factors_df, left_index=True, right_index=True, direction='backward')
        if 'DXY_close' in df_merged.columns:
            df_merged['DXY_return_1d'] = df_merged['DXY_close'].pct_change(periods=1)
            df_merged['DXY_return_5d'] = df_merged['DXY_close'].pct_change(periods=5)
        if 'VIX_close' in df_merged.columns:
            df_merged['VIX_SMA_20'] = df_merged['VIX_close'].rolling(window=20).mean()
            if 'VIX_SMA_20' in df_merged.columns and df_merged['VIX_SMA_20'].notna().any():
                 df_merged['VIX_vs_SMA20'] = (df_merged['VIX_close'] - df_merged['VIX_SMA_20']) / (df_merged['VIX_SMA_20'] + 1e-9)
        self.logger.debug("Added global factor features (DXY, VIX).")
        return df_merged

    def _add_base_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ åŸºç¤æŠ€è¡“åˆ†æç‰¹å¾µ"""
        df_finta = df.copy()
        df_finta.rename(columns={'tick_volume': 'volume'}, inplace=True)
        for p in [5, 14, 20]: df_finta[f'ATR_{p}'] = TA.ATR(df_finta, period=p)
        df_finta = df_finta.join(TA.DMI(df_finta))
        periods = [10, 20, 50]
        for indicator in ['SMA', 'EMA', 'RSI']:
            method = getattr(TA, indicator)
            if indicator == 'RSI': df_finta[f'{indicator}_14'] = method(df_finta, period=14)
            else:
                for p in periods: df_finta[f'{indicator}_{p}'] = method(df_finta, period=p)
        df_finta = df_finta.join(TA.STOCH(df_finta)); df_finta = df_finta.join(TA.MACD(df_finta)); df_finta = df_finta.join(TA.BBANDS(df_finta))
        df_finta['body_size'] = abs(df_finta['close'] - df_finta['open'])
        df_finta['upper_wick'] = df_finta['high'] - df_finta[['open', 'close']].max(axis=1)
        df_finta['lower_wick'] = df_finta[['open', 'close']].min(axis=1) - df_finta['low']
        df_finta['body_vs_wick'] = df_finta['body_size'] / (df_finta['high'] - df_finta['low'] + 1e-9)
        if 'DI+' in df_finta.columns and 'DI-' in df_finta.columns: df_finta['DMI_DIFF'] = abs(df_finta['DI+'] - df_finta['DI-'])
        if 'ATR_14' in df_finta.columns:
            atr_series = df_finta['ATR_14'] + 1e-9
            df_finta['body_size_norm'] = df_finta['body_size'] / atr_series
            if 'DMI_DIFF' in df_finta.columns: df_finta['DMI_DIFF_norm'] = df_finta['DMI_DIFF'] / atr_series
        df_finta.rename(columns={'volume': 'tick_volume'}, inplace=True)
        return df_finta

    def _add_market_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ–°å¢å¸‚å ´å¾®çµæ§‹ç‰¹å¾µ,ä»¥æ•æ‰Kæ£’å…§çš„è²·è³£åŠ›é“"""
        df_out = df.copy()
        df_out['buying_pressure'] = (df_out['close'] - df_out['low']) / (df_out['high'] - df_out['low'] + 1e-9)
        net_change = abs(df_out['close'].diff(20)); total_movement = df_out['close'].diff().abs().rolling(20).sum()
        df_out['efficiency_ratio'] = net_change / (total_movement + 1e-9)
        ofi_proxy = (df_out['tick_volume'] * np.sign(df_out['close'].diff())).fillna(0)
        df_out['ofi_cumsum_20'] = ofi_proxy.rolling(20).sum()
        return df_out

    def _add_regime_detection_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ–°å¢å¸‚å ´ç‹€æ…‹è­˜åˆ¥ç‰¹å¾µ,ä»¥åˆ¤æ–·è¶¨å‹¢/ç›¤æ•´åŠæ³¢å‹•ç‹€æ…‹"""
        df_out = df.copy()
        returns = df_out['close'].pct_change(); df_out['volatility_20p'] = returns.rolling(20).std()
        vol_mean_60 = df_out['volatility_20p'].rolling(60).mean(); vol_std_60 = df_out['volatility_20p'].rolling(60).std()
        df_out['vol_regime_zscore'] = (df_out['volatility_20p'] - vol_mean_60) / (vol_std_60 + 1e-9)
        try:
            adx_df = TA.ADX(df_out, period=14)
            df_out['adx_14'] = adx_df['ADX'] if isinstance(adx_df, pd.DataFrame) else adx_df
            df_out['trend_strength'] = (df_out['adx_14'] / 100).fillna(0)
        except Exception as e:
            self.logger.warning(f"ADX calculation failed: {e}. Using alternative method.")
            ma_20 = df_out['close'].rolling(20).mean(); trend_slope = ma_20.diff(10) / ma_20.shift(10)
            df_out['trend_strength'] = abs(trend_slope).fillna(0); df_out['adx_14'] = df_out['trend_strength'] * 100
        conditions = [(df_out['vol_regime_zscore'] > 1) & (df_out['trend_strength'] > 0.3), (df_out['vol_regime_zscore'] > 1) & (df_out['trend_strength'] <= 0.3),
                      (df_out['vol_regime_zscore'] <= 1) & (df_out['trend_strength'] > 0.3), (df_out['vol_regime_zscore'] <= 1) & (df_out['trend_strength'] <= 0.3)]
        choices = [3, 2, 1, 0]; df_out['market_regime'] = np.select(conditions, choices, default=0)
        return df_out

    def _add_advanced_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ é«˜ç´šæ™‚é–“ç‰¹å¾µ"""
        df_out = df.copy(); hour = df_out.index.hour
        df_out['day_of_week'] = df_out.index.dayofweek; df_out['hour_of_day'] = hour; df_out['month_of_year'] = df_out.index.month
        df_out['is_tokyo_session'] = ((hour >= 0) & (hour <= 8)).astype(int); df_out['is_london_session'] = ((hour >= 8) & (hour <= 16)).astype(int)
        df_out['is_ny_session'] = ((hour >= 13) & (hour <= 21)).astype(int); df_out['is_london_ny_overlap'] = ((hour >= 13) & (hour <= 16)).astype(int)
        df_out = pd.get_dummies(df_out, columns=['day_of_week', 'hour_of_day'], prefix=['dow', 'hour'])
        return df_out

    def _add_price_action_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼·åƒ¹æ ¼è¡Œç‚ºç‰¹å¾µ"""
        df_out = df.copy()
        if 'SMA_50' in df_out.columns and 'ATR_14' in df_out.columns: df_out['price_vs_sma50_norm'] = (df_out['close'] - df_out['SMA_50']) / (df_out['ATR_14'] + 1e-9)
        if all(col in df_out.columns for col in ['BB_MIDDLE', 'BB_UPPER', 'BB_LOWER']):
            bb_width = df_out['BB_UPPER'] - df_out['BB_LOWER'] + 1e-9
            df_out['price_vs_bb'] = (df_out['close'] - df_out['BB_MIDDLE']) / bb_width * 2
        window = 50
        df_out[f'bars_since_{window}p_high'] = df_out['high'].rolling(window).apply(lambda x: len(x) - np.argmax(x) - 1, raw=True)
        df_out[f'bars_since_{window}p_low'] = df_out['low'].rolling(window).apply(lambda x: len(x) - np.argmin(x) - 1, raw=True)
        return df_out

    def _add_advanced_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼·æ³¢å‹•ç‡ç‰¹å¾µ"""
        df_out = df.copy()
        if 'ATR_5' in df_out.columns and 'ATR_20' in df_out.columns: df_out['atr_ratio_5_20'] = df_out['ATR_5'] / (df_out['ATR_20'] + 1e-9)
        returns = df_out['close'].pct_change()
        upside_returns, downside_returns = returns.where(returns > 0, 0), returns.where(returns < 0, 0)
        df_out['upside_vol_20p'] = upside_returns.rolling(20).std()
        df_out['downside_vol_20p'] = abs(downside_returns).rolling(20).std()
        df_out['vol_asymmetry'] = df_out['upside_vol_20p'] / (df_out['downside_vol_20p'] + 1e-9)
        df_out[['upside_vol_20p', 'downside_vol_20p', 'vol_asymmetry']] = df_out[['upside_vol_20p', 'downside_vol_20p', 'vol_asymmetry']].fillna(method='ffill')
        return df_out

    def _add_multi_period_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ å¤šé€±æœŸç‰¹å¾µ"""
        df_out = df.copy()
        for n in [5, 10, 20, 60]:
            df_out[f'returns_{n}p'] = df_out['close'].pct_change(periods=n)
            returns = df_out['close'].pct_change()
            df_out[f'volatility_{n}p'] = returns.rolling(window=n).std() * np.sqrt(n)
        return df_out

    def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼·äº’å‹•ç‰¹å¾µ"""
        df_out = df.copy()
        if 'RSI_14' in df_out.columns and 'volatility_20p' in df_out.columns: df_out['RSI_x_volatility'] = (df_out['RSI_14'] - 50) * df_out['volatility_20p']
        if 'vol_regime_zscore' in df_out.columns and 'trend_strength' in df_out.columns: df_out['regime_x_trend'] = df_out['vol_regime_zscore'] * df_out['trend_strength']
        if 'RSI_14' in df_out.columns and 'price_vs_sma50_norm' in df_out.columns: df_out['rsi_x_pullback'] = (df_out['RSI_14'] - 50) * df_out['price_vs_sma50_norm']
        if 'vol_asymmetry' in df_out.columns and 'market_regime' in df_out.columns: df_out['asymmetry_x_regime'] = df_out['vol_asymmetry'] * df_out['market_regime']
        return df_out

    def _add_event_features(self, df: pd.DataFrame, events_df: Optional[pd.DataFrame], symbol: str) -> pd.DataFrame:
        """æ·»åŠ ç¶“æ¿Ÿäº‹ä»¶ç‰¹å¾µ"""
        df_out = df.copy()
        if events_df is None or events_df.empty: df_out['time_to_next_event'], df_out['next_event_importance'] = 999, 0; return df_out
        currency1, currency2 = symbol[:3].upper(), symbol[3:6].upper()
        relevant_events = events_df[events_df['currency'].isin([currency1, currency2])].copy()
        if relevant_events.empty: df_out['time_to_next_event'], df_out['next_event_importance'] = 999, 0; return df_out
        relevant_events.reset_index(inplace=True)
        df_merged = pd.merge_asof(left=df_out.sort_index(), right=relevant_events.sort_values('timestamp_utc'), left_index=True, right_on='timestamp_utc', direction='forward')
        df_merged.index = df_out.index
        time_diff = df_merged['timestamp_utc'] - df_out.index
        df_out['time_to_next_event'] = time_diff.dt.total_seconds() / 3600; df_out['next_event_importance'] = df_merged['importance_val']
        df_out.fillna({'time_to_next_event': 999, 'next_event_importance': 0}, inplace=True)
        return df_out

    def process_symbol_group(self, symbol: str, paths: Dict[str, Path], events_df: Optional[pd.DataFrame], factors_df: Optional[pd.DataFrame]):
        self.logger.info(f"--- é–‹å§‹è™•ç†å•†å“çµ„: {symbol} ---"); dataframes = {}
        for tf in self.config.TIMEFRAME_ORDER:
            if tf in paths:
                self.logger.info(f"[{symbol}] æ­£åœ¨è™•ç† {tf} æ•¸æ“š...")
                df = pd.read_parquet(paths[tf])
                try:
                    df = self._add_global_factor_features(df, factors_df); df = self._add_event_features(df, events_df, symbol); df = self._add_base_features(df)
                    df = self._add_market_microstructure_features(df); df = self._add_regime_detection_features(df); df = self._add_advanced_volatility_features(df)
                    df = self._add_price_action_features(df); df = self._add_multi_period_features(df); df = self._add_advanced_time_features(df); df = self._add_interaction_features(df)
                    if tf == 'D1': df['is_uptrend'] = (df['close'] > df['SMA_50']).astype(int)
                    dataframes[tf] = df
                except Exception as e: self.logger.error(f"[{symbol}/{tf}] ç‰¹å¾µè¨ˆç®—éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}"); continue
        final_dataframes = {}; cols_to_drop = ['open', 'high', 'low', 'close', 'tick_volume', 'real_volume', 'spread']
        if 'D1' in dataframes: final_dataframes['D1'] = dataframes['D1']
        if 'H4' in dataframes and 'D1' in dataframes:
            df_d1_renamed = dataframes['D1'].rename(columns=lambda c: f"D1_{c}" if c not in cols_to_drop else c)
            final_dataframes['H4'] = pd.merge_asof(dataframes['H4'].sort_index(), df_d1_renamed.drop(columns=cols_to_drop, errors='ignore'), left_index=True, right_index=True, direction='backward')
        elif 'H4' in dataframes: final_dataframes['H4'] = dataframes['H4']
        if 'H1' in dataframes and 'H4' in final_dataframes:
            df_h4_renamed = final_dataframes['H4'].rename(columns=lambda c: f"H4_{c}" if c not in cols_to_drop else c)
            final_dataframes['H1'] = pd.merge_asof(dataframes['H1'].sort_index(), df_h4_renamed.drop(columns=cols_to_drop, errors='ignore'), left_index=True, right_index=True, direction='backward')
        elif 'H1' in dataframes: final_dataframes['H1'] = dataframes['H1']
        for tf, df_final in final_dataframes.items():
            df_final.replace([np.inf, -np.inf], np.nan, inplace=True); rows_before = len(df_final)
            df_final.fillna(method='ffill', inplace=True); df_final.dropna(inplace=True); rows_after = len(df_final)
            if rows_before > rows_after: self.logger.info(f"[{symbol}/{tf}] æ¸…ç†äº† {rows_before - rows_after} è¡Œåˆå§‹ç„¡æ•ˆæ•¸æ“šã€‚")
            if rows_after == 0 and rows_before > 0: self.logger.error(f"[{symbol}/{tf}] åš´é‡éŒ¯èª¤ï¼šæ•¸æ“šè¢«å®Œå…¨æ¸…ç©ºï¼"); continue
            try:
                relative_path = paths[tf].relative_to(self.config.INPUT_BASE_DIR); output_path = self.config.OUTPUT_BASE_DIR / relative_path
                output_path.parent.mkdir(parents=True, exist_ok=True); df_final.to_parquet(output_path)
                self.logger.info(f"[{symbol}/{tf}] å·²å„²å­˜ç¶œåˆç‰¹å¾µæª”æ¡ˆåˆ°: {output_path} ({df_final.shape[0]:,} è¡Œ, {df_final.shape[1]} åˆ—)")
            except Exception as e: self.logger.error(f"[{symbol}/{tf}] ä¿å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def run(self):
        self.logger.info(f"========= ç¶œåˆç‰¹å¾µå·¥ç¨‹æµç¨‹é–‹å§‹ (v8.0) =========")
        events_df, factors_df = None, None
        try:
            events_df = pd.read_csv(self.config.EVENTS_FILE_PATH, index_col='timestamp_utc', parse_dates=True); events_df.index = pd.to_datetime(events_df.index, utc=True)
            self.logger.info(f"æˆåŠŸè®€å–äº‹ä»¶æ•¸æ“š: {len(events_df)} ç­†")
        except FileNotFoundError: self.logger.warning(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°äº‹ä»¶æª”æ¡ˆ {self.config.EVENTS_FILE_PATH}ï¼")
        try:
            factors_df = pd.read_parquet(self.config.GLOBAL_FACTORS_FILE_PATH); factors_df.index = pd.to_datetime(factors_df.index, utc=True)
            self.logger.info(f"æˆåŠŸè®€å–å…¨å±€å› å­æ•¸æ“š: {factors_df.shape}")
        except FileNotFoundError: self.logger.warning(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°å…¨å±€å› å­æª”æ¡ˆ {self.config.GLOBAL_FACTORS_FILE_PATH}ï¼")
        input_files = list(self.config.INPUT_BASE_DIR.rglob("*.parquet"))
        if not input_files: self.logger.warning("åœ¨è¼¸å…¥ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• Parquet æª”æ¡ˆã€‚"); return
        symbol_groups = defaultdict(dict)
        for file_path in input_files:
            try:
                parts = file_path.stem.split('_'); symbol, timeframe = '_'.join(parts[:-1]), parts[-1]
                if timeframe in self.config.TIMEFRAME_ORDER: symbol_groups[symbol][timeframe] = file_path
            except IndexError: self.logger.warning(f"ç„¡æ³•è§£ææª”å: {file_path.name}ï¼Œå·²è·³éã€‚")
        self.logger.info(f"å…±æ‰¾åˆ° {len(symbol_groups)} å€‹å•†å“çµ„éœ€è¦è™•ç†ã€‚")
        for symbol, paths in symbol_groups.items():
            try: self.process_symbol_group(symbol, paths, events_df, factors_df)
            except Exception as e: self.logger.error(f"è™•ç†å•†å“çµ„ {symbol} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        self.logger.info(f"========= ç‰¹å¾µå·¥ç¨‹æµç¨‹çµæŸ =========")

# ==============================================================================
#                      3. ç‰¹å¾µåµéŒ¯å™¨é¡åˆ¥
# ==============================================================================
class FeatureDebugger:
    """ç”¨æ–¼åˆ†æç‰¹å¾µå·¥ç¨‹å¾Œæ•¸æ“šå¥åº·ç‹€æ³çš„åµéŒ¯å™¨"""
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        # ç¢ºä¿æ—¥èªŒè¨˜éŒ„å™¨å·²è¨­å®š
        if not self.logger.hasHandlers():
            self.logger.setLevel(self.config.LOG_LEVEL.upper())
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            sh = logging.StreamHandler(sys.stdout)
            sh.setFormatter(formatter)
            self.logger.addHandler(sh)

    def run_health_check(self, market_name: str):
        """å°æŒ‡å®šçš„å¸‚å ´æ¨£æœ¬åŸ·è¡Œå¥åº·åº¦æª¢æŸ¥"""
        self.logger.info(f"\n{'='*80}\nğŸš€ é–‹å§‹å°æ¨£æœ¬å¸‚å ´ [{market_name}] é€²è¡Œç‰¹å¾µå¥åº·åº¦æª¢æŸ¥...\n{'='*80}")
        
        market_folder = "_".join(market_name.split('_')[:2])
        data_file = self.config.OUTPUT_BASE_DIR / market_folder / f"{market_name}.parquet"

        if not data_file.exists():
            self.logger.error(f"âŒ åµéŒ¯å¤±æ•—ï¼šæ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ: {data_file}")
            return

        try:
            df = pd.read_parquet(data_file)
            self.logger.info(f"âœ… æˆåŠŸè¼‰å…¥ç‰¹å¾µæ•¸æ“š: {df.shape}")
        except Exception as e:
            self.logger.error(f"âŒ åµéŒ¯å¤±æ•—ï¼šè®€å–ç‰¹å¾µæª”æ¡ˆæ™‚å‡ºéŒ¯: {e}")
            return
            
        # --- å¥åº·åº¦åˆ†æ ---
        rows_total = len(df)
        nan_count = df.isnull().sum().sum()
        inf_count = np.isinf(df.select_dtypes(include=np.number)).sum().sum()
        
        print("\nğŸ“Š --- æœ€çµ‚æ•¸æ“šå¥åº·åº¦å ±å‘Š ---")
        print(f"   ç¸½è¡Œæ•¸: {rows_total:,}")
        print(f"   ç¸½ç‰¹å¾µæ•¸: {df.shape[1]}")
        
        if nan_count > 0: print(f"   âš ï¸  NaN (ç¼ºå¤±å€¼) ç¸½æ•¸: {nan_count:,}")
        else: print(f"   âœ… NaN (ç¼ºå¤±å€¼) ç¸½æ•¸: 0")
            
        if inf_count > 0: print(f"   âš ï¸  Infinite (ç„¡é™å€¼) ç¸½æ•¸: {inf_count:,}")
        else: print(f"   âœ… Infinite (ç„¡é™å€¼) ç¸½æ•¸: 0")
        
        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨å®Œå…¨ç‚ºç©ºæˆ–å…¨ç‚ºNaNçš„æ¬„ä½
        empty_cols = [col for col in df.columns if df[col].isnull().all()]
        if empty_cols:
            self.logger.warning(f"ğŸš¨ åš´é‡è­¦å‘Šï¼šä»¥ä¸‹æ¬„ä½å®Œå…¨ç‚ºç©º (å…¨æ˜¯NaN)ï¼Œè«‹æª¢æŸ¥å…¶è¨ˆç®—é‚è¼¯: {empty_cols}")
        else:
            self.logger.info("âœ… æ‰€æœ‰æ¬„ä½éƒ½åŒ…å«æœ‰æ•ˆæ•¸æ“šã€‚")
            
        print(f"{'='*80}\nğŸš€ å¥åº·åº¦æª¢æŸ¥å®Œç•¢ã€‚\n{'='*80}")

# ==============================================================================
#                      4. ä¸»åŸ·è¡Œå€å¡Š
# ==============================================================================
if __name__ == "__main__":
    try:
        config = Config()
        
        # --- æ­¥é©Ÿ 1: åŸ·è¡Œç‰¹å¾µå·¥ç¨‹ ---
        engineer = FeatureEngineer(config)
        engineer.run()
        
        # --- æ­¥é©Ÿ 2: åŸ·è¡Œè‡ªå‹•åµéŒ¯ ---
        debugger = FeatureDebugger(config)
        debugger.run_health_check(market_name=config.DEBUG_MARKET_SAMPLE)
        
    except Exception as e:
        logging.critical(f"è…³æœ¬åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        sys.exit(1)
